{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Actividad 2: Reinforcement Learning: **Frozen lake problem**"
      ],
      "metadata": {
        "id": "3qMnOMrtnwtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grupo 8:\n",
        "- Ricardo Castillo Rodríguez\n",
        "- Miriam Santana\n",
        "- Katherine Escobar\n"
      ],
      "metadata": {
        "id": "J4d_Yp6jeevx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción al problema de FrozenLake ❄️"
      ],
      "metadata": {
        "id": "A1y1axECz5u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entorno FrozenLake es un clásico problema de Reinforcement Learning desarrollado por OpenAI Gym.\n",
        "\n",
        "Se representa como una cuadrícula de 4x4 donde:\n",
        "- `S` marca la posición de inicio (Start).\n",
        "- `F` son celdas seguras (Frozen - Hielo seguro).\n",
        "- `H` son agujeros peligrosos (Holes) que hacen fallar el episodio.\n",
        "- `G` es la meta final (Goal).\n",
        "\n",
        "El objetivo del agente es aprender a desplazarse desde `S` hasta `G`:\n",
        "- Evitando caer en los agujeros.\n",
        "- Tomando el menor número de pasos posible.\n",
        "\n",
        "En esta actividad trabajaremos con la versión **no resbaladiza** (`is_slippery=False`), donde los movimientos son totalmente deterministas (el agente se mueve exactamente en la dirección que elige).\n",
        "\n",
        "Este entorno nos permitirá entrenar un agente utilizando algoritmos de aprendizaje por refuerzo, concretamente **Q-Learning**, para que descubra la mejor estrategia para alcanzar el objetivo de forma segura y eficiente.\n",
        "\n",
        "Documentación: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n"
      ],
      "metadata": {
        "id": "jHZ9tHzJz8zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos, instrucciones y evaluación"
      ],
      "metadata": {
        "id": "f2PNlKXAp6p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objetivos\n",
        "- Conseguir movermos aleatoriamente hasta cumplir el objetivo\n",
        "- Conseguir que el agente aprenda con Q-learning\n",
        "- (Opcional) Probar con otros hiperparámetros\n",
        "- (Opcional) Modificar la recompensa\n",
        "\n",
        "Consideraciones\n",
        "- No hay penalizaciones\n",
        "- Si el agente cae en un \"hole\", entonces done = True y se queda atascado sin poder salir (al igual que ocurre cuando llega al \"goal\")\n",
        "\n",
        "Normas a seguir\n",
        "\n",
        "- Se debe entregar un **ÚNICO GOOGLE COLAB notebook** (archivo .ipynb) que incluya las instrucciones presentes y su **EJECUCIÓN!!!**.\n",
        "- Poner el nombre del grupo en el nombre del archivo y el nombre de todos los integrantes del grupo al inicio del notebook.\n",
        "\n",
        "Criterio de evaluación\n",
        "\n",
        "- Seguimiento de las normas establecidas en la actividad.\n",
        "- Corrección en el uso de algoritmos, modelos y formas idiomáticas en Python.\n",
        "- El código debe poder ejecutarse sin modificación alguna en Google Colaboratory."
      ],
      "metadata": {
        "id": "Sg9aSbNap5VC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalamos librerías**"
      ],
      "metadata": {
        "id": "9Ly604VTn1ue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXbvm0y9No4N",
        "outputId": "253f17ea-a827-447d-b6dd-4eb0654360f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.24.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install gym==0.17.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "import random as rd"
      ],
      "metadata": {
        "id": "-aiKby2RNy-T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Definición del entorno**"
      ],
      "metadata": {
        "id": "shd3NqyQn9IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el entorno\n",
        "env = gym.make('FrozenLake-v0', desc=None, map_name=\"4x4\", is_slippery=False)"
      ],
      "metadata": {
        "id": "1_3z3ZByoAcO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fijamos una semilla\n",
        "seed_value = 42\n",
        "env.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "import random\n",
        "random.seed(seed_value)"
      ],
      "metadata": {
        "id": "E_Nw22y00NEH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset() # En este caso, empieza desde la misma posición inicial\n",
        "print(env.render())"
      ],
      "metadata": {
        "id": "fETKbHsBOGtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931b186c-5412-47f2-db77-584a731a96d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0p9Zxwz0UNs",
        "outputId": "37351d4f-0c2e-4379-fe2f-8a5a6c87fe25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space Discrete(4)\n",
            "State Space Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acciones posibles:\n",
        "* 0: izquierda\n",
        "* 1: abajo\n",
        "* 2: derecha\n",
        "* 3: arriba"
      ],
      "metadata": {
        "id": "JarMsz_-0YfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificador de estado\n",
        "state = env.s\n",
        "print(\"State:\", state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyya51AU0shk",
        "outputId": "ddeceffc-f753-46c9-db1a-1af49587dd3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **¡Nos movemos aleatoriamente!**"
      ],
      "metadata": {
        "id": "jAHw0ZBm1C1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itxDrfce3-x0",
        "outputId": "19d78e03-4e8e-4be0-a642-f535a9a90320"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Acciones: 0=izquierda, 1=abajo, 2=derecha, 3=arriba\n",
        "action = 1\n",
        "state, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"State:\", state)\n",
        "print(state, reward, done, info)\n",
        "\n",
        "env.s = state\n",
        "env.render()\n",
        "\n",
        "steps += 1\n",
        "\n",
        "print(f\"Step: {steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5EoVvD61Gjb",
        "outputId": "5296fd98-c015-4255-d0eb-0c9b85197586"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 4\n",
            "4 0.0 False {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Step: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de episodios que vamos a jugar\n",
        "episodes = 10\n",
        "\n",
        "# Bucle para cada episodio\n",
        "for episode in range(episodes):\n",
        "    # Reiniciamos el entorno al inicio de cada episodio\n",
        "    state = env.reset()\n",
        "    done = False  # Indicador de si hemos terminado el episodio\n",
        "    steps = 0     # Contador de pasos realizados\n",
        "\n",
        "    print(f\"Episode {episode+1}\")\n",
        "    sleep(1)\n",
        "\n",
        "\n",
        "    while not done: # Mientras no hayamos terminado (ni caído en agujero ni alcanzado la meta)\n",
        "        action = env.action_space.sample() # Elegimos una acción aleatoria (exploración total)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) # Aplicamos la acción y recogemos la información resultante\n",
        "\n",
        "        clear_output(wait=True) # Mostramos visualmente el entorno tras cada movimiento\n",
        "        env.render()\n",
        "\n",
        "        state = next_state # Actualizamos el estado actual\n",
        "\n",
        "        steps += 1 # Aumentamos el contador de pasos\n",
        "\n",
        "        sleep(0.5) # Pequeña pausa para ver mejor la simulación\n",
        "\n",
        "    # Mensaje al finalizar el episodio\n",
        "    if reward == 1:\n",
        "        print(f\"¡Objetivo conseguido en {steps} pasos!\")\n",
        "    else:\n",
        "        print(f\"Caíste en un agujero después de {steps} pasos.\")\n",
        "\n",
        "    sleep(1)"
      ],
      "metadata": {
        "id": "s9LDqu1ar56Q",
        "outputId": "03e2923e-80f5-4435-f29f-a3bcfaf28e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Caíste en un agujero después de 2 pasos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1️⃣ **Primer intento**"
      ],
      "metadata": {
        "id": "WzDPvfoMv8Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenamiento: Q-Learning**"
      ],
      "metadata": {
        "id": "dFY0vnxCtxqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos la Q-Table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Definimos hiperparámetros\n",
        "alpha = 0.8    # tasa de aprendizaje\n",
        "gamma = 0.95   # factor de descuento\n",
        "epsilon = 0.1  # tasa de exploración (ε-greedy)\n",
        "episodes = 10000  # número de episodios de entrenamiento\n",
        "\n",
        "# Entrenamiento\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Política ε-greedy: explora o explota\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Exploración\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Explotación\n",
        "\n",
        "        # Ejecutamos acción\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Penalización por paso para que entienda que cuantos menos pasos mejor y para que no vuelva sobre los pasos ya dados\n",
        "        reward = reward - 0.01\n",
        "\n",
        "        # Actualizamos Q-Table usando la fórmula Q-Learning\n",
        "        q_table[state, action] = q_table[state, action] + alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        # Actualizamos estado\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "yeXXqW4iscvl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluación: Resultados de Q-Learning inicial**"
      ],
      "metadata": {
        "id": "qG6xBPZpuXON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_episodes = 10\n",
        "successes = 0\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    print(f\"Episode {episode+1}\")\n",
        "    sleep(1)\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        sleep(0.5)\n",
        "\n",
        "    if reward == 1:\n",
        "        print(f\"¡Objetivo conseguido en {steps} pasos!\")\n",
        "        successes += 1\n",
        "    else:\n",
        "        print(f\"Caíste en un agujero después de {steps} pasos.\")\n",
        "\n",
        "    sleep(1)\n",
        "\n",
        "# Mostramos porcentaje de éxito\n",
        "print(f\"\\nPorcentaje de éxito: {successes/test_episodes*100:.2f}%\")"
      ],
      "metadata": {
        "id": "KPtFXdOyt8t9",
        "outputId": "8d1235f8-a7bb-444a-e153-4124e5411e99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Caíste en un agujero después de 2 pasos.\n",
            "\n",
            "Porcentaje de éxito: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que el agente no consigue llegar al objetivo."
      ],
      "metadata": {
        "id": "8Y0y6D-9wKTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Análisis**"
      ],
      "metadata": {
        "id": "OmUj5--pwT0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que FrozenLake es un entorno con alta dificultad por la disposición de los agujeros, decidimos ajustar los hiperparámetros para mejorar el aprendizaje del agente:\n",
        "\n",
        "Acciones tomadas:\n",
        "\n",
        "- Incrementar el epsilon para fomentar más exploración.\n",
        "- Aumentar el número de episodios.\n",
        "- Reducir ligeramente la penalización por paso.\n",
        "\n",
        "Con los nuevos hiperparámetros, esperamos que el agente explore más, descubra caminos exitosos y reduzca el número de caídas en agujeros.\n",
        "\n"
      ],
      "metadata": {
        "id": "uelP_taPwbzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2️⃣ **Segundo intento**"
      ],
      "metadata": {
        "id": "aVMFhMAqw_Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenamiento**"
      ],
      "metadata": {
        "id": "4nV7DiWvxC0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos la Q-Table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Definimos hiperparámetros\n",
        "alpha = 0.8    # tasa de aprendizaje\n",
        "gamma = 0.95   # factor de descuento\n",
        "epsilon = 0.5  # tasa de exploración (ε-greedy)\n",
        "episodes = 30000  # número de episodios de entrenamiento\n",
        "\n",
        "# Entrenamiento\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Política ε-greedy: explora o explota\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Exploración\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Explotación\n",
        "\n",
        "        # Ejecutamos acción\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Penalización por paso\n",
        "        reward = reward - 0.001\n",
        "\n",
        "        # Actualizamos Q-Table usando la fórmula Q-Learning\n",
        "        q_table[state, action] = q_table[state, action] + alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n",
        "        )\n",
        "\n",
        "        # Actualizamos estado\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "C92ndEifxBtv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluación**"
      ],
      "metadata": {
        "id": "fKoGUQdcxi-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_episodes = 10\n",
        "successes = 0\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    print(f\"Episode {episode+1}\")\n",
        "    sleep(1)\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        sleep(0.5)\n",
        "\n",
        "    if reward == 1:\n",
        "        print(f\"¡Objetivo conseguido en {steps} pasos!\")\n",
        "        successes += 1\n",
        "    else:\n",
        "        print(f\"Caíste en un agujero después de {steps} pasos.\")\n",
        "\n",
        "    sleep(1)\n",
        "\n",
        "# Mostramos porcentaje de éxito\n",
        "print(f\"\\nPorcentaje de éxito: {successes/test_episodes*100:.2f}%\")"
      ],
      "metadata": {
        "id": "l06W0Cn4x470",
        "outputId": "80bbe5d5-b2e3-420e-ac34-91a73d1676a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "¡Objetivo conseguido en 6 pasos!\n",
            "\n",
            "Porcentaje de éxito: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de entrenar con la nueva configuración (mayor `epsilon`, más episodios y penalización de paso reducida), evaluamos el agente.\n",
        "\n",
        "Resultados observados:\n",
        "- ¡Objetivo conseguido en todos los episodios!\n",
        "- El agente logra encontrar el camino al objetivo sin caer en agujeros.\n",
        "\n",
        "**Porcentaje de éxito: 100%**\n",
        "\n",
        "Esto demuestra una clara mejora en el desempeño del agente respecto al primer intento, confirmando que los ajustes de hiperparámetros fueron acertados.\n"
      ],
      "metadata": {
        "id": "2RUJS2unyiRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusión final**"
      ],
      "metadata": {
        "id": "LQdQ0CcByQGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso de iteración, análisis de errores y ajuste de hiperparámetros permitió que el agente aprendiera a resolver el entorno FrozenLake de manera eficiente.\n",
        "\n",
        "Esto resalta la importancia de:\n",
        "- Fomentar suficiente exploración durante el entrenamiento.\n",
        "- Permitir que el agente aprenda de sus errores a lo largo de más episodios.\n",
        "- Ajustar correctamente las penalizaciones para guiar el comportamiento deseado.\n",
        "\n",
        "En definitiva, el agente fue capaz de alcanzar el objetivo en el 100% de las pruebas tras la mejora, reflejando un aprendizaje exitoso."
      ],
      "metadata": {
        "id": "IK21P-uwzyoc"
      }
    }
  ]
}